{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d13d60d-1e91-452a-9186-28dd18b99c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045d8d01-992f-4468-9092-87dee3e669cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_clf = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da17d30-d91f-4c60-91e8-9aaa3e60c7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9927969574928284}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_clf(\"Fortunately robots clean the house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2024f2a0-988c-453f-ad60-b0b6a6af5356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.7485972046852112}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_clf(\"robots clean the house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01fc561a-4458-41bb-a84f-11ef1ae60cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997004270553589}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_clf(\"Unfortunately robots doesn't clean the house perfect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15af19e8-880d-4443-811a-9b2fe847c0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9859856367111206}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_clf(\"I clean the house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a444e8c-63f1-446a-b38f-432996ac6ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", model = \"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2427053f-ce6a-4ad0-9cfe-173b1b0369d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.9655575,\n",
       "  'index': 4,\n",
       "  'word': 'Sofia',\n",
       "  'start': 11,\n",
       "  'end': 16},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9996573,\n",
       "  'index': 18,\n",
       "  'word': 'Greece',\n",
       "  'start': 70,\n",
       "  'end': 76}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"My name is Sofia, I am a robot and am working as a human assistant in Greece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f75f4956-5b4c-484a-a81b-16a2d97bec73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8af04c6108947e8b2f5be843cbbb18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\llms_course_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68533d84004441f69543f418e31061b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5189284009940eba3410eaf26c02453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38899d81ea84165aa01816038029b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f782dbc1fc82412e88eec1322a35fa91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7289b6e9ccd94c4f95c9081e65495c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "zeroshot_clf =  pipeline(\"zero-shot-classification\", model = \"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd2757a2-5f13-43c7-aed7-085c57c6b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"one day I will become a Chef and travel all over the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7068cfd7-795e-4699-b8e5-61b59be93354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will become a Chef and travel all over the world',\n",
       " 'labels': ['travel', 'cooking', 'dancing'],\n",
       " 'scores': [0.570115327835083, 0.4274391233921051, 0.0024455718230456114]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshot_clf(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c95e076a-6cec-426b-9ec5-d174a2139197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained Tokenizers\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fe66dae-6422-43d8-a352-43c0c2401fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84c587d8-60e3-47cd-90c3-8fc807cc0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'm so excited to be learning about large language models. GenAI is the best.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "390ec552-f4d2-4c95-a9b6-5c79b3edbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fun(model, sentence):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    input_ids = tokenizer(sentence)\n",
    "    print(input_ids)\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    print(tokens)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(token_ids)\n",
    "    decoded_ids = tokenizer.decode(token_ids)\n",
    "    print(decoded_ids)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b947888e-955a-4c49-ac32-b578f181eadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 1049, 2061, 7568, 2000, 2022, 4083, 2055, 2312, 2653, 4275, 1012, 8991, 4886, 2003, 1996, 2190, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['i', \"'\", 'm', 'so', 'excited', 'to', 'be', 'learning', 'about', 'large', 'language', 'models', '.', 'gen', '##ai', 'is', 'the', 'best', '.']\n",
      "[1045, 1005, 1049, 2061, 7568, 2000, 2022, 4083, 2055, 2312, 2653, 4275, 1012, 8991, 4886, 2003, 1996, 2190, 1012]\n",
      "i ' m so excited to be learning about large language models. genai is the best.\n"
     ]
    }
   ],
   "source": [
    "tokenize_fun(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42a62052-2513-4d01-a951-585fd28cce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [35, 26, 98, 102, 5564, 22, 39, 1899, 75, 392, 1243, 2626, 9, 3159, 246, 96, 27, 18, 252, 9, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['▁I', \"'\", 'm', '▁so', '▁excited', '▁to', '▁be', '▁learning', '▁about', '▁large', '▁language', '▁models', '.', '▁Gen', 'A', 'I', '▁is', '▁the', '▁best', '.']\n",
      "[35, 26, 98, 102, 5564, 22, 39, 1899, 75, 392, 1243, 2626, 9, 3159, 246, 96, 27, 18, 252, 9]\n",
      "I'm so excited to be learning about large language models. GenAI is the best.\n"
     ]
    }
   ],
   "source": [
    "model2 = \"xlnet-base-cased\"\n",
    "tokenize_fun(model2, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b3ff7-8fc9-45ca-bfc5-a5f16e5ae3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
